{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCN Assignment 6: Generative\tAdversarial\tNetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.datasets import TupleDataset\n",
    "from chainer.functions.evaluation import accuracy\n",
    "from chainer.functions.loss import softmax_cross_entropy\n",
    "from chainer import link\n",
    "from chainer import reporter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make loss function such that loss is high when discriminator \n",
    "# classifies generator network output as fake 0, and\n",
    "# low when generator network output as being a real 0\n",
    "\n",
    "class Generator(Chain):\n",
    "    def __init__(self, n_units, dim_output):\n",
    "        super(Generator, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(n_units, n_units) # fully connected layer\n",
    "            self.batch_norm = L.BatchNormalization(n_units) # batch normalization\n",
    "            self.deconv = L.Deconvolution2D(None, (dim_output, dim_output)) # deconvolution layer\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = self.batch_norm(h1)\n",
    "        h3 = self.deconv(h2)\n",
    "        y = F.sigmoid(h3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(Chain):\n",
    "    def __init__(self, n_out):\n",
    "        super(Discriminator, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(None, out_channels=5, ksize=5, stride=1, pad=0)\n",
    "            self.fc1 = L.Linear(None, n_out)\n",
    "            \n",
    "        def __call__(self, x):\n",
    "            h1 = self.conv1(x)\n",
    "            h2 = F.max_pooling_2d(h1, ksize=5, stride=1, pad=0) # max pooling layer\n",
    "            y = F.relu(self.fc1(h2))\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(generator, discriminator, classifier, optimizer, true_train_iter, true_val_iter, fake_train_iter):\n",
    "    classifier.cleargrads()\n",
    "    \n",
    "#     First generate a sample with the generator network.\n",
    "#     Classify it with the discriminator network, \n",
    "#     calculate the loss such that you enhance samples that the discriminator thinks are real \n",
    "#     and update the networks. \n",
    "#     Next calculate the loss of the generated sample enhancing those samples \n",
    "#     that the discriminator correctly recognizes as fake. \n",
    "#     Combine this with the loss that the discriminator gets on real images \n",
    "#     and update the networks based on this combined loss.\n",
    "    \n",
    "    # get next mini-batch\n",
    "    batch = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # generate the real data\n",
    "    train_data, test_data = get_mnist(n_train=1000, n_test=100, with_label=False, classes = [0])\n",
    "    \n",
    "    # generate the fake data\n",
    "    fake_train = np.random.rand(1000,28,28)\n",
    "#     fake_test = np.random.rand(100,28,28)\n",
    "    \n",
    "    # initialize interators\n",
    "    true_train_iter = iterators.SerialIterator(train_data, batch_size=64, shuffle=True)\n",
    "    true_val_iter = iterators.SerialIterator(test_data, batch_size=100, repeat=False, shuffle=False)\n",
    "    fake_train_iter = iterators.SerialIterator(fake_train, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # define models\n",
    "    generator = Generator(784, 28)\n",
    "    discriminator = Discriminator(1)\n",
    "    classifier = L.Classifier(discriminator, F.sigmoid_cross_entropy)\n",
    "    optimizer = optimizers.SGD()\n",
    "    optimizer.setup(classifier)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
